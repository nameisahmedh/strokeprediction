from flask import Flask, render_template, request, jsonify, session, redirect
import pandas as pd
import numpy as np
import stroke_ml as ml
import os
import io
import base64
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns

app = Flask(__name__)
app.secret_key = 'stroke_prediction_secret_key_2024'
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024
os.makedirs('uploads', exist_ok=True)

session_data = {}

@app.route('/')
def index():
    return render_template('home.html')

@app.route('/upload-page')
def upload_page():
    return render_template('upload.html')

@app.route('/train-page')
def train_page():
    if 'raw_df' not in session_data:
        return redirect('/upload-page')
    if 'meta' not in session_data:
        return redirect('/preprocess-page')
    return render_template('train.html')

@app.route('/predict-page')
def predict_page():
    if 'raw_df' not in session_data:
        return redirect('/upload-page')
    if 'meta' not in session_data:
        return redirect('/preprocess-page')
    if 'model' not in session_data:
        return redirect('/train-page')
    return render_template('predict.html')

@app.route('/analysis-page')
def analysis_page():
    if 'raw_df' not in session_data:
        return redirect('/upload-page')
    if 'meta' not in session_data:
        return redirect('/preprocess-page')
    if 'model' not in session_data:
        return redirect('/train-page')
    return render_template('analysis.html')

@app.route('/learn-more')
def learn_more():
    return render_template('learn_more.html')

@app.route('/preprocess-page')
def preprocess_page():
    if 'raw_df' not in session_data:
        return redirect('/upload-page')
    return render_template('preprocess.html')

@app.route('/check_data_status')
def check_data_status():
    return jsonify({
        'data_loaded': 'raw_df' in session_data,
        'preprocessed': 'meta' in session_data,
        'model_trained': 'model' in session_data
    })

@app.route('/download_analysis_pdf')
def download_analysis_pdf():
    if 'raw_df' not in session_data:
        return jsonify({'error': 'No data uploaded'}), 400
    
    try:
        from reportlab.lib.pagesizes import letter
        from reportlab.pdfgen import canvas
        from reportlab.lib.units import inch
        import io
        from datetime import datetime
        
        buffer = io.BytesIO()
        c = canvas.Canvas(buffer, pagesize=letter)
        width, height = letter
        
        # Title
        c.setFont("Helvetica-Bold", 24)
        c.drawString(inch, height - inch, "Arix Stroke Prediction")
        c.setFont("Helvetica-Bold", 18)
        c.drawString(inch, height - 1.5*inch, "Analysis Report")
        
        # Date
        c.setFont("Helvetica", 12)
        c.drawString(inch, height - 2*inch, f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Statistics
        df = session_data['raw_df']
        c.setFont("Helvetica-Bold", 14)
        c.drawString(inch, height - 2.7*inch, "Dataset Statistics:")
        c.setFont("Helvetica", 12)
        c.drawString(inch, height - 3.1*inch, f"Total Records: {len(df)}")
        c.drawString(inch, height - 3.4*inch, f"Total Features: {len(df.columns)}")
        
        if 'stroke' in df.columns:
            stroke_count = df['stroke'].sum()
            no_stroke_count = len(df) - stroke_count
            c.drawString(inch, height - 3.7*inch, f"Stroke Cases: {stroke_count}")
            c.drawString(inch, height - 4*inch, f"No Stroke Cases: {no_stroke_count}")
        
        c.drawString(inch, height - 4.5*inch, f"Missing Values: {df.isna().sum().sum()}")
        
        # Note
        c.setFont("Helvetica-Oblique", 10)
        c.drawString(inch, inch, "Generated by Arix Stroke Prediction System")
        
        c.save()
        buffer.seek(0)
        
        return send_file(
            buffer,
            mimetype='application/pdf',
            as_attachment=True,
            download_name=f'analysis_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.pdf'
        )
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/get_analysis', methods=['POST'])
def get_analysis():
    if 'raw_df' not in session_data:
        return jsonify({'error': 'Please upload a dataset first (Step 1: Upload Data)'}), 400
    
    try:
        df = session_data['raw_df']
        
        # Dataset statistics
        stats = {
            'total_rows': int(df.shape[0]),
            'total_columns': int(df.shape[1]),
            'missing_values': int(df.isna().sum().sum()),
            'stroke_cases': int(df['stroke'].sum()) if 'stroke' in df.columns else 0,
            'no_stroke_cases': int((df['stroke'] == 0).sum()) if 'stroke' in df.columns else 0
        }
        
        # Class distribution chart
        if 'stroke' in df.columns:
            fig, ax = plt.subplots(figsize=(6, 4))
            df['stroke'].value_counts().plot(kind='bar', ax=ax, color=['#10B981', '#EF4444'])
            ax.set_xlabel('Stroke')
            ax.set_ylabel('Count')
            ax.set_title('Class Distribution')
            ax.set_xticklabels(['No Stroke', 'Stroke'], rotation=0)
            buf = io.BytesIO()
            plt.savefig(buf, format='png', bbox_inches='tight')
            buf.seek(0)
            class_dist_img = base64.b64encode(buf.read()).decode()
            plt.close()
        else:
            class_dist_img = None
        
        # Age distribution
        if 'age' in df.columns:
            fig, ax = plt.subplots(figsize=(8, 4))
            ax.hist(df['age'], bins=20, color='#2563EB', alpha=0.7, edgecolor='black')
            ax.set_xlabel('Age')
            ax.set_ylabel('Frequency')
            ax.set_title('Age Distribution')
            buf = io.BytesIO()
            plt.savefig(buf, format='png', bbox_inches='tight')
            buf.seek(0)
            age_dist_img = base64.b64encode(buf.read()).decode()
            plt.close()
        else:
            age_dist_img = None
        
        # Correlation heatmap
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 1:
            fig, ax = plt.subplots(figsize=(10, 8))
            corr = df[numeric_cols].corr()
            sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', ax=ax, center=0)
            ax.set_title('Feature Correlation Heatmap')
            buf = io.BytesIO()
            plt.savefig(buf, format='png', bbox_inches='tight')
            buf.seek(0)
            corr_img = base64.b64encode(buf.read()).decode()
            plt.close()
        else:
            corr_img = None
        
        return jsonify({
            'stats': stats,
            'class_dist_img': class_dist_img,
            'age_dist_img': age_dist_img,
            'corr_img': corr_img
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/get_feature_importance', methods=['POST'])
def get_feature_importance():
    if 'model' not in session_data or 'meta' not in session_data:
        return jsonify({'error': 'Please train a model first (Step 3: Train Model)'}), 400
    
    try:
        model = session_data['model']
        meta = session_data['meta']
        
        # Get feature importance if available
        if hasattr(model, 'feature_importances_'):
            importances = model.feature_importances_
            selected_indices = meta['selector'].get_support(indices=True)
            all_feature_names = meta['feature_names']
            
            # Get feature names for selected features only
            selected_feature_names = [all_feature_names[i] for i in selected_indices]
            
            # Create bar chart
            fig, ax = plt.subplots(figsize=(10, 6))
            indices = np.argsort(importances)[::-1]
            ax.bar(range(len(importances)), importances[indices], color='#2563EB')
            ax.set_xlabel('Features')
            ax.set_ylabel('Importance')
            ax.set_title('Feature Importance')
            ax.set_xticks(range(len(importances)))
            ax.set_xticklabels([selected_feature_names[i] for i in indices], rotation=45, ha='right')
            
            buf = io.BytesIO()
            plt.savefig(buf, format='png', bbox_inches='tight')
            buf.seek(0)
            importance_img = base64.b64encode(buf.read()).decode()
            plt.close()
            
            return jsonify({'importance_img': importance_img})
        else:
            return jsonify({'error': 'Model does not support feature importance'}), 400
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/use_default_dataset', methods=['POST'])
def use_default_dataset():
    try:
        df = pd.read_csv('Dataset/healthcare-dataset-stroke-data.csv')
        session_data['raw_df'] = df
        
        stats = {
            'rows': int(df.shape[0]),
            'columns': int(df.shape[1]),
            'missing': int(df.isna().sum().sum()),
            'preview': df.head(10).to_html(classes='table table-striped', index=False)
        }
        return jsonify(stats)
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/use_default_test', methods=['POST'])
def use_default_test():
    if 'model' not in session_data or 'meta' not in session_data:
        return jsonify({'error': 'Please train or load a model first (Step 3: Train Model)'}), 400
    
    try:
        test_path = os.path.join('Dataset', 'testData.csv')
        if not os.path.exists(test_path):
            return jsonify({'error': f'Test file not found at {test_path}'}), 400
        
        df = pd.read_csv(test_path)
        preds = ml.predict_dataframe(session_data['model'], session_data['meta'], df)
        
        df['predicted_stroke'] = preds
        df['stroke_risk'] = df['predicted_stroke'].map({0: 'No Risk', 1: 'High Risk'})
        
        output = io.StringIO()
        df.to_csv(output, index=False)
        output.seek(0)
        
        return jsonify({
            'total': int(len(df)),
            'strokes': int((preds == 1).sum()),
            'preview': df.head(10).to_html(classes='table table-striped', index=False),
            'csv': output.getvalue()
        })
    except Exception as e:
        import traceback
        return jsonify({'error': str(e), 'details': traceback.format_exc()}), 400

@app.route('/upload', methods=['POST'])
def upload_dataset():
    if 'file' not in request.files:
        return jsonify({'error': 'No file uploaded'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No file selected'}), 400
    
    try:
        df = pd.read_csv(file)
        session_data['raw_df'] = df
        
        stats = {
            'rows': int(df.shape[0]),
            'columns': int(df.shape[1]),
            'missing': int(df.isna().sum().sum()),
            'preview': df.head(10).to_html(classes='table table-striped', index=False)
        }
        return jsonify(stats)
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/preprocess', methods=['POST'])
def preprocess():
    if 'raw_df' not in session_data:
        return jsonify({'error': 'Upload dataset first'}), 400
    
    try:
        meta = ml.preprocess_df(session_data['raw_df'])
        session_data['meta'] = meta
        
        return jsonify({
            'train_size': meta['X_train'].shape[0],
            'test_size': meta['X_test'].shape[0],
            'features': meta['X_train'].shape[1]
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/train', methods=['POST'])
def train_model():
    if 'raw_df' not in session_data:
        return jsonify({'error': 'Please upload a dataset first (Step 1: Upload Data)'}), 400
    if 'meta' not in session_data:
        return jsonify({'error': 'Please run preprocessing first (Step 2: Preprocess Data)'}), 400
    
    model_name = request.json.get('model', 'RandomForest')
    
    try:
        meta = session_data['meta']
        model, metrics = ml.train_and_evaluate(
            model_name, meta['X_train'], meta['y_train'], 
            meta['X_test'], meta['y_test']
        )
        
        session_data['model'] = model
        session_data['metrics'] = metrics
        
        # Generate confusion matrix image
        cm = metrics['confusion_matrix']
        fig, ax = plt.subplots(figsize=(6, 5))
        sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', ax=ax,
                   xticklabels=['No Stroke', 'Stroke'],
                   yticklabels=['No Stroke', 'Stroke'])
        ax.set_xlabel('Predicted')
        ax.set_ylabel('Actual')
        
        buf = io.BytesIO()
        plt.savefig(buf, format='png', bbox_inches='tight')
        buf.seek(0)
        cm_img = base64.b64encode(buf.read()).decode()
        plt.close()
        
        return jsonify({
            'accuracy': float(metrics['accuracy']),
            'precision': float(metrics['precision']),
            'recall': float(metrics['recall']),
            'f1': float(metrics['f1']),
            'confusion_matrix': cm_img
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/compare', methods=['POST'])
def compare_models():
    if 'raw_df' not in session_data:
        return jsonify({'error': 'Please upload a dataset first (Step 1: Upload Data)'}), 400
    if 'meta' not in session_data:
        return jsonify({'error': 'Please run preprocessing first (Step 2: Preprocess Data)'}), 400
    
    try:
        meta = session_data['meta']
        names = ['RandomForest', 'LogisticRegression', 'SVM', 'KNN', 'NaiveBayes']
        if ml.HAS_XGBOOST: names.append('XGBoost')
        if ml.HAS_CATBOOST: names.append('CatBoost')
        
        results = ml.compare_models(names, meta['X_train'], meta['y_train'], 
                                   meta['X_test'], meta['y_test'])
        
        comparison = []
        for name, res in results.items():
            if 'error' not in res:
                comparison.append({
                    'model': name,
                    'accuracy': float(res['accuracy']),
                    'precision': float(res['precision']),
                    'recall': float(res['recall']),
                    'f1': float(res['f1'])
                })
        
        return jsonify({'results': comparison})
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/predict', methods=['POST'])
def predict():
    if 'model' not in session_data or 'meta' not in session_data:
        return jsonify({'error': 'Please train or load a model first (Step 3: Train Model)'}), 400
    
    data = request.json
    meta = session_data['meta']
    
    try:
        row = []
        for fname in meta['feature_names']:
            val = data.get(fname, 0)
            if fname in meta['encoders']:
                le = meta['encoders'][fname]
                val_str = str(val)
                if val_str in le.classes_:
                    mapped = le.transform([val_str])[0]
                else:
                    mapped = 0
                row.append(mapped)
            else:
                row.append(float(val))
        
        arr = np.array([row])
        arr_scaled = meta['scaler'].transform(arr)
        arr_sel = meta['selector'].transform(arr_scaled)
        pred = session_data['model'].predict(arr_sel)[0]
        
        proba = None
        if hasattr(session_data['model'], 'predict_proba'):
            proba = session_data['model'].predict_proba(arr_sel)[0].tolist()
        
        risk_level = 'No Risk' if pred == 0 else 'High Risk'
        confidence = (proba[int(pred)] * 100) if proba else None
        
        return jsonify({
            'prediction': int(pred),
            'risk_level': risk_level,
            'probability': proba,
            'confidence': confidence
        })
    except Exception as e:
        import traceback
        return jsonify({'error': str(e), 'trace': traceback.format_exc()}), 400

@app.route('/predict_csv', methods=['POST'])
def predict_csv():
    if 'model' not in session_data or 'meta' not in session_data:
        return jsonify({'error': 'Please train or load a model first (Step 3: Train Model)'}), 400
    
    if 'file' not in request.files:
        return jsonify({'error': 'No file uploaded'}), 400
    
    try:
        file = request.files['file']
        df = pd.read_csv(file)
        
        # Add missing columns with default values
        required_cols = session_data['meta']['feature_names']
        for col in required_cols:
            if col not in df.columns:
                df[col] = 0
        
        preds = ml.predict_dataframe(session_data['model'], session_data['meta'], df)
        
        df['predicted_stroke'] = preds
        df['stroke_risk'] = df['predicted_stroke'].map({0: 'No Risk', 1: 'High Risk'})
        
        output = io.StringIO()
        df.to_csv(output, index=False)
        output.seek(0)
        
        return jsonify({
            'total': len(df),
            'strokes': int((preds == 1).sum()),
            'preview': df.head(10).to_html(classes='table table-striped', index=False),
            'csv': output.getvalue()
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 400

@app.route('/save_model', methods=['POST'])
def save_model():
    if 'model' not in session_data or 'meta' not in session_data:
        return jsonify({'error': 'No trained model to save. Please train a model first.'}), 400
    
    try:
        save_path = 'stroke_model.pkl'
        ml.save_artifact(save_path, session_data['model'], session_data['meta'])
        return jsonify({
            'success': True,
            'message': f'Model saved successfully to {save_path}',
            'features': len(session_data['meta']['feature_names'])
        })
    except Exception as e:
        return jsonify({'error': f'Save failed: {str(e)}'}), 400

@app.route('/load_model', methods=['POST'])
def load_model():
    try:
        load_path = 'stroke_model.pkl'
        
        if not os.path.exists(load_path):
            return jsonify({'error': 'No saved model found. Please train and save a model first.'}), 400
        
        artifact = ml.load_artifact(load_path)
        session_data['model'] = artifact['model']
        session_data['meta'] = artifact['meta']
        
        return jsonify({
            'success': True,
            'message': f'Model loaded successfully from {load_path}',
            'features': len(artifact['meta']['feature_names'])
        })
    except Exception as e:
        return jsonify({'error': f'Load failed: {str(e)}'}), 400

if __name__ == '__main__':
    app.run(debug=True, port=5000)
